__all__ = [
    "create_content", "update_content", "create_key_func", "transfer_value"
]


import base64
import logging
from email.parser import BytesParser

import requests
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from cryptography.hazmat.primitives.serialization import (
    load_der_private_key, load_der_public_key
)
from django.core.files.base import ContentFile, File
from django.db import transaction
from django.db.models import Q
from django.test import Client
from graphql_relay import from_global_id
from rdflib import Graph

from ....constants import TransferResult
from ....utils.auth import retrieve_allowed_objects
from ....utils.conf import get_requests_params
from ....utils.encryption import default_padding, encrypt_into_file
from ....utils.misc import calculate_hashes, hash_object, get_secrets
from ...models import Cluster, Content, ContentReference, ContentTag
from ._actions import create_actions_func

logger = logging.getLogger(__name__)

len_default_hash = len(hash_object(b""))


def _transform_info_tags(objdata, is_key):
    info_tags = []
    key_hashes = set()
    for i in set(objdata.get("info") or []):
        if i.startswith("id="):
            logger.warning("id is an invalid tag (autogenerated)")
            continue
        if i in {"public"}:
            logger.warning("public is an invalid tag (autogenerated)")
            continue
        elif not is_key and i in {"public_key", "private_key"}:
            raise ValueError("key is an invalid tag (autogenerated)")
        elif i.startswith("key_hash="):
            key_hash = i.split("=")[-1]
            if len_default_hash == len(key_hash):
                key_hashes.add(key_hash)
        if len(i) > 8000:
            raise ValueError("Info tag too big")
        info_tags.append(ContentTag(tag=i))
    return info_tags, key_hashes


def _transform_key_into_dataobj(key_obj, key=None, content=None):
    if isinstance(key_obj.get("privateKey"), str):
        key_obj["privateKey"] = base64.b64decode(key_obj["privateKey"])
    if isinstance(key_obj["publicKey"], str):
        key_obj["publicKey"] = base64.b64decode(key_obj["publicKey"])
    if isinstance(key_obj.get("nonce"), str):
        key_obj["nonce"] = base64.b64decode(key_obj["nonce"])
    if key and key_obj.get("privateKey"):
        if not key_obj.get("nonce"):
            raise ValueError("encrypted private key requires nonce")
        aesgcm = AESGCM(key)
        privkey = aesgcm.decrypt(
            key_obj["privateKey"],
            key_obj["nonce"],
            None
        )
        privkey = load_der_private_key(privkey, None, default_backend())
        key_obj["privateKey"] = aesgcm.encrypt(
            privkey.private_bytes(
                encoding=serialization.Encoding.DER,
                format=serialization.PrivateFormat.PKCS8,
                encryption_algorithm=serialization.NoEncryption()
            ),
            key_obj["nonce"],
            None
        )

        key_obj["publicKey"] = privkey.public_bytes(
            encoding=serialization.Encoding.DER,
            format=serialization.PublicFormat.SubjectPublicKeyInfo,
            encryption_algorithm=serialization.NoEncryption()
        )
    else:
        try:
            pubkey = load_der_public_key(
                key_obj["publicKey"], None, default_backend()
            )
            key_obj["publicKey"] = pubkey.public_bytes(
                encoding=serialization.Encoding.DER,
                format=serialization.PublicFormat.SubjectPublicKeyInfo,
                encryption_algorithm=serialization.NoEncryption()
            )
        except Exception as exc:
            # logger.debug("loading public key failed", exc_info=exc)
            raise ValueError("Invalid public key") from exc
    if content:
        if content.value.open("rb").read() != key_obj["publicKey"]:
            raise ValueError("Cannot change public key")
    hashes = calculate_hashes(key_obj["publicKey"])
    info = list(map(
        lambda x: f"key_hash={x}", hashes
    ))

    return (
        hashes,
        {
            "nonce": "",
            "value": key_obj["publicKey"],
            "info": ["publicKey"].extend(info),
            "contentHash": hashes[0]
        },
        {
            "nonce": key_obj["nonce"],
            "value": key_obj["privateKey"],
            "info": ["privateKey"].extend(info),
            "contentHash": None
        } if key_obj.get("privateKey") else None
    )


def _update_or_create_content_or_key(
    request, content, objdata, authset, is_key, required_keys
):
    if isinstance(objdata["cluster"], str):
        type_name, objdata["cluster"] = from_global_id(objdata["cluster"])
        if type_name != "Cluster":
            raise ValueError("Requires Cluster id")
        objdata["cluster"] = retrieve_allowed_objects(
            request, "update", Cluster.objects.filter(
                flexid=objdata["cluster"]
            ), authset=authset
        )["objects"].first()
    if objdata.get("cluster"):
        content.cluster = objdata["cluster"]
    if not content.cluster:
        raise ValueError()

    create = not content.id

    # if create checked in parent function
    if objdata.get("value"):
        # normalize nonce and check constraints
        try:
            if isinstance(objdata["nonce"], bytes):
                checknonce = objdata["nonce"]
                objdata["nonce"] = base64.b64encode(checknonce)
            else:
                checknonce = base64.b64decode(objdata["nonce"])
        except Exception:
            # no nonce == trigger encryption
            objdata["value"], objdata["nonce"], objdata["key"] = \
                encrypt_into_file(
                    objdata["value"],
                    key=objdata.get("key") or None
                )
        if len(checknonce) != 13:
            raise ValueError("invalid nonce size")
        if checknonce.count(b"\0") == len(checknonce):
            raise ValueError("weak nonce")
        content.nonce = objdata["nonce"]

        if isinstance(objdata["value"], bytes):
            f = ContentFile(objdata["value"])
        elif isinstance(objdata["value"], str):
            f = ContentFile(base64.b64decode(objdata["value"]))
        else:
            f = File(objdata["value"])

        def save_func_value():
            content.file.delete(False)
            content.file.save("", f)
    else:
        def save_func_value():
            content.save()

    final_info_tags = None
    if objdata.get("info") is not None:
        final_info_tags, key_hashes_info = \
            _transform_info_tags(objdata, is_key)
    else:
        key_hashes_info = set()

    chash = objdata.get("contentHash")
    if chash is not None:
        if len(chash) not in (0, len_default_hash):
            raise ValueError("Invalid hashing algorithm used for content_hash")
        if len(chash) == 0:
            content.content_hash = None
        else:
            content.content_hash = chash

    final_references = None
    key_hashes_ref = set()
    if objdata.get("references") is not None:
        final_references = []
        for ref in objdata["references"]:
            if isinstance(ref["target"], Content):
                targetob = ref["target"]
            else:
                targetob = Content.objects.filter(
                    Q(id=ref["target"]) |
                    Q(flexid=ref["target"]),
                    mark_for_destruction=None
                ).first()
            if not targetob:
                continue
            if ref.get("extra") and len(ref["extra"]) > 8000:
                raise ValueError("Extra tag too big")
            refob = ContentReference(
                target=targetob, group=ref.get("group") or "",
                extra=ref.get("extra") or ""
            )
            if refob.group in {"key", "transfer"}:
                refob.delete_recursive = None
                if refob.group == "key":
                    key_hashes_ref.add(targetob.content_hash)
                if targetob.content_hash not in key_hashes_info:
                    raise ValueError("Key hash not found in info")
            final_references.append(refob)
    elif create:
        final_references = []

    inner_key = objdata.get("key")
    if not is_key and not key_hashes_ref and inner_key:
        if isinstance(inner_key, str):
            inner_key = base64.b64decode(inner_key)
        # last resort
        if final_references is not None:
            default_keys = retrieve_allowed_objects(
                request, "view", Content.objects.filter(
                    cluster=content.cluster,
                    info__tag="public_key", authset=authset
                )
            )["objects"]

            if required_keys:
                default_keys |= Content.objects.filter(
                    info__tag="public_key",
                    info__tag__in=map(
                        lambda x: f"key_hash={x}",
                        required_keys
                    )
                )

            for keyob in default_keys.distinct():
                refob = ContentReference(
                    target=keyob, group="key", delete_recursive=None,
                    extra=keyob.encrypt(
                        inner_key,
                        default_padding
                    )
                )
                final_references.append(refob)
                key_hashes_info.add(keyob.content_hash)
                final_info_tags.append(ContentTag(
                    tag=f"key_hash={keyob.content_hash}"
                ))

    if is_key and len(key_hashes_info) < 1:
        raise ValueError(
            ">=1 key_hash info tags required for key (for action key)"
        )
    elif not is_key and len(key_hashes_ref) < 1:
        raise ValueError(
            ">=1 key required for content"
        )
    elif len(key_hashes_info) < 2:
        raise ValueError(
            "missing key_hash info tag (for action key)"
        )
    elif not key_hashes_info.issuperset(required_keys):
        raise ValueError(
            "missing required keys"
        )
    if objdata.get("actions") is not None:
        actions_save_func = create_actions_func(
            content, objdata["actions"], request
        )
    else:
        def actions_save_func():
            pass

    def save_func():
        save_func_value()
        if final_info_tags is not None:
            # simply ignore id=, can only be changed in regenerateFlexid
            if not create:
                content.info.exclude(
                    Q(tag__startswith="id=") |
                    Q(tag="public")
                ).delete()

            content.info.create_bulk(final_info_tags)

        # create id tag after object was created or update it
        content.info.update_or_create(
            defaults={"tag": f"id={content.flexid}"},
            tag__startswith="id="
        )
        if final_references is not None:
            if not create:
                if is_key:
                    refs = content.references.exclude(group="private_key")
                else:
                    refs = content.references.all()
                refs.delete()
            content.references.create_bulk(final_references)
            if content.cluster.public:
                if not is_key:
                    if content.references.filter(
                        group="key",
                        target__info="public"
                    ):
                        content.info.update_or_create(tag="public")
                    else:
                        content.info.filter(tag="public").delete()
                else:
                    g = Graph()
                    g.parse(content.cluster.public_info, "turtle")
                    if not key_hashes_info.isdisjoint(
                        map(hash_object, get_secrets(g)[0])
                    ):
                        content.info.update_or_create(tag="public")
                    else:
                        content.info.filter(tag="public").delete()
        actions_save_func()
    return content
    return save_func


def create_key_func(
    request, objdata, key=None, authset=None
):
    key_obj = objdata.get("key")
    if not key_obj:
        raise ValueError("Requires key")
    if isinstance(objdata["cluster"], str):
        type_name, objdata["cluster"] = from_global_id(objdata["cluster"])
        if type_name != "Cluster":
            raise ValueError("Requires Cluster id")
        objdata["cluster"] = retrieve_allowed_objects(
            request, "view", Cluster.objects.filter(
                flexid=objdata["cluster"]
            ), authset=authset
        )["objects"].first()
    if not objdata["cluster"]:
        raise ValueError()

    hashes, public, private = _transform_key_into_dataobj(key_obj, key=key)
    public_content = None
    if objdata["cluster"].id:
        public_content = Content.objects.filter(
            cluster=objdata["cluster"],
            info__tag="public_key",
            info__tag__in=map(lambda x: f"key_hash={x}", hashes)
        ).first()
    public_content = public_content or Content()
    public["info"].extend(objdata.get("info") or [])
    public["actions"] = objdata.get("actions")
    if private:
        private["info"].extend(objdata.get("info") or [])
        private["actions"] = objdata.get("actions")
        private = _update_or_create_content_or_key(
            request, Content(), private, authset, True, []
        )
        private["references"].append({
            "target": public_content,
            "group": "publicKey",
            "delete_recursive": True
        })
    public = _update_or_create_content_or_key(
        request, public_content, public, authset, True, []
    )

    def func():
        return public(), private and private()

    return func


def create_content(
    request, objdata, key=None, authset=None, required_keys=None
):
    value_obj = objdata.get("value", {})
    key_obj = objdata.get("key")
    if not value_obj and not key_obj:
        raise ValueError("Requires value or key")
    if value_obj and key_obj:
        raise ValueError("Can only specify one of value or key")

    is_key = False
    if key_obj:
        is_key = True
        save_func = create_key_func(
            request, objdata, key=key, authset=authset
        )

        with transaction.atomic():
            return save_func()[0]
    else:
        newdata = {
            "cluster": objdata.get("cluster"),
            "references": objdata.get("references"),
            "contentHash": objdata.get("contentHash"),
            "info": objdata.get("info"),
            "actions": objdata.get("actions"),
            "key": key,
            **value_obj
        }
        content_obj = Content()
        save_func = _update_or_create_content_or_key(
            request, content_obj, newdata, authset, is_key,
            required_keys or []
        )

        with transaction.atomic():
            return save_func()


def update_content(
    request, content, objdata, key=None, authset=None,
    required_keys=None
):
    assert content.id
    is_key = False
    if content.info.filter(tag="public_key"):
        is_key = True
        key_obj = objdata.get("key")
        if not key_obj:
            raise ValueError("Cannot transform key to content")

        hashes, newdata, private = _transform_key_into_dataobj(
            key_obj, key=key, content=content
        )
        newdata["info"].extend(objdata.get("info") or [])
    elif content.info.filter(tag="private_key"):
        is_key = True
        key_obj = objdata.get("key")
        if not key_obj:
            raise ValueError("Cannot transform key to content")

        hashes, public, newdata = _transform_key_into_dataobj(
            key_obj, key=key, content=content
        )
        if not newdata:
            raise ValueError()
        newdata["info"].extend(objdata.get("info") or [])
    else:
        newdata = {
            "cluster": objdata.get("cluster"),
            "references": objdata.get("references"),
            "contentHash": objdata.get("contentHash"),
            "info": objdata.get("info"),
            "key": key,
            **objdata.get("value", {})
        }
    newdata["actions"] = objdata.get("actions")
    func = _update_or_create_content_or_key(
        request, content, newdata, authset, is_key,
        required_keys or []
    )
    with transaction.atomic():
        return func()


def transfer_value(
    content, key=None, url=None, headers=None, transfer=True
):
    _headers = {}
    if key:
        assert not url, "can only specify key or url"
        try:
            _blob = AESGCM(key).decrypt(
                content.value.open("rb").read(),
                base64.b64decode(content.nonce),
                None
            ).split(b'\r\n', 1)
            if len(_blob) == 1:
                url = _blob[0]
            else:
                url = _blob[0]
                _headers.update(
                    BytesParser().parsebytes(_blob[1], headersonly=True)
                )
        except Exception as exc:
            logger.error("Error while decoding url, headers", exc_info=exc)
            return TransferResult.ERROR

    if headers:
        _headers.update(headers)

    params, inline_domain = get_requests_params(url)
    # block content while updating file
    q = Q(id=content.id)
    if transfer:
        q &= Q(info__tag="transfer")
    bcontents = Content.objects.filter(
        q
    ).select_for_update()
    with transaction.atomic():
        # 1. lock content, 2. check if content was deleted before updating
        if not bcontents:
            return TransferResult.ERROR
        if inline_domain:
            response = Client().get(
                url,
                Connection="close",
                SERVER_NAME=inline_domain,
                **_headers
            )
            if response.status_code == 404:
                return TransferResult.NOTFOUND
            elif response.status_code != 200:
                return TransferResult.ERROR
            try:
                with content.value.open("wb") as f:
                    for chunk in response.streaming_content:
                        f.write(chunk)
                if transfer:
                    content.references.filter(group="transfer").delete()
                return TransferResult.SUCCESS
            except Exception as exc:
                logger.error("Error while transferring content", exc_info=exc)
            return TransferResult.ERROR
        else:
            try:
                with requests.get(
                    url,
                    headers={
                        "Connection": "close",
                        **_headers
                    },
                    **params
                ):
                    if response.status_code == 404:
                        return TransferResult.NOTFOUND
                    elif response.status_code != 200:
                        return TransferResult.ERROR
                    with content.value.open("wb") as f:
                        for chunk in response.iter_content(512):
                            f.write(chunk)
                    if transfer:
                        content.references.filter(group="transfer").delete()
                    return TransferResult.SUCCESS
            except Exception as exc:
                logger.error("Error while transferring content", exc_info=exc)
            return TransferResult.ERROR
